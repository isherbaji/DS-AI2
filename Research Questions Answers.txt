R1. Cross-Modality Consistency of SHAP Explanations: How consistently do SHAP local and global explanations identify the most influential input features across tabular, text, and image prediction tasks?

This research question investigates the consistency of local and global SHAP (SHapley Additive exPlanations) in identifying influential input features across three distinct data modalities: tabular, text, and image data. Our findings indicate that the efficacy and interpretability of SHAP explanations vary significantly with the data type, demonstrating high utility for tabular and text data but limited applicability for image-based models in our specific implementation.

Tabular Data:

For tabular datasets, SHAP explanations demonstrated the highest level of consistency and interpretability. The discrete and well-defined nature of features in tabular data allows SHAP to effectively quantify the importance of each variable individually.

Global explanation were highly effective for explainability, providing a clear overview of the average impact of each feature on the model's predictions across the entire dataset. This is instrumental in understanding the model's general behavior.
Local explanation were also exceptionally useful, offering precise insights into how each feature contributed to a specific, individual prediction. The use of waterfall diagrams to visualize these local contributions was particularly beneficial for explainability.


Textual Data:
In the context of our binary sentiment classification, SHAP explanations provided valuable insights into feature importance at both the global and local levels.

Global and Local Word Importance: SHAP successfully identified specific words that had a significant general effect on predictions (global explanation). It was also effective at the local level, highlighting which words in a given text contributed most to its specific classification, thereby enhancing the model's explainability.

Token Positional Importance: An exploratory analysis was conducted to determine the importance of token position (e.g., beginning, middle, end) on model predictions. This approach yielded less interpretable results. However, we acknowledge a limitation in our methodology; the text was divided into 256 tokens, which may be too granular to capture broad positional effects. Future research could investigate whether segmenting the text into a smaller number of tokens (e.g., three, to represent beginning, middle, and end) would yield more meaningful insights into positional feature importance.


Image Data:
For image classification tasks, our findings suggest that SHAP, in the manner it was implemented in our study, offers limited value for explainability. Our analysis focused on evaluating average pixel importance to identify which regions of an image were most influential in the model's predictions.

Across different classes of objects, including cats, dogs, airplanes, and automobiles, a consistent pattern emerged: pixels located in the center of the image were identified as the most important. This outcome is likely an artifact of the dataset itself, where the primary subject of the image is typically centered. Consequently, instead of providing insight into the specific features of the object that the model is identifying (e.g., the ears of a dog or the wings of a plane), the SHAP analysis merely highlighted the location of the object within the frame. This provides little in the way of true explainability regarding the model's decision-making process based on visual features.




R2. Local Fidelity Under Feature Removal: To what extent does removing the top-k features ranked by SHAP affect model performance?


Tabular Data: XGBoost Model:

The removal of the top three most influential features, as identified by SHAP, resulted in a marginal decline in model performance. The initial weighted average performance was a precision of 0.58, recall of 0.59, and an F1-Score of 0.54. After feature removal, this changed to a precision of 0.57, recall of 0.58, and an F1-Score of 0.50. This slight degradation, while minor, aligns with the expected outcome, confirming that the features identified by SHAP are material to the model's predictive capability. The modest magnitude of this performance drop can likely be attributed to two factors: the model's initial performance metrics were not exceptionally high, and the number of removed features (k=3) was small. Removing a larger number of top-ranked features would likely have induced a more substantial decline.

Textual Data: DistilBERT Model:

In contrast to the tabular model, the feature removal experiment on the DistilBERT sentiment classification model yielded no meaningful change in performance. After removing the top 10 most influential words for both positive and negative sentiment classifications, the model's metrics remained virtually static, with accuracy changing from 0.9079 to 0.9076 and the F1-Score from 0.9094 to 0.9091. A plausible explanation for this resilience lies in the inherent nature of transformer-based language models. DistilBERT's predictions are not solely dependent on a few high-weight keywords but leverage complex contextual relationships between tokens. The model's understanding of sentiment is distributed across a wide array of features, including syntax and co-occurrence. Therefore, the removal of a small number of top-ranked words is likely compensated for by other contextual cues and redundant features within the text, leaving the model's overall predictive accuracy intact. This suggests a high degree of feature redundancy and robustness in the language model.

Image Data: ResNet-50 Model:

The feature removal experiment was not conducted for the ResNet-50 image classification model. This decision was based on a conceptual challenge in defining and removing "features" from image data in a meaningful way. In this context, SHAP identifies the importance of individual pixels. Unlike features in tabular or text data, individual pixels do not correspond to semantic concepts. The removal of top-k pixels (e.g., by setting their value to zero) would introduce significant artifacts and disrupt the local spatial structures—such as edges, textures, and shapes—that the convolutional neural network (CNN) relies upon for classification. Such an operation would fundamentally alter the input's integrity in a manner not analogous to removing a feature column or a word token, thereby rendering any resulting performance comparison invalid.